{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from einops import rearrange"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that calculate the loss of GPT-2. Given\n",
    "- `xb` is a batch of text\n",
    "- `yb` is a batch of target tokens for each sentence in `xb`\n",
    "\n",
    "`gpt_model` outputs `logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model():\n",
    "    def inner(x):\n",
    "        logits, _ = model(x)\n",
    "        return logits\n",
    "    return inner\n",
    "\n",
    "gpt_model = new_model()\n",
    "\n",
    "def compute_loss(model, xb, yb):\n",
    "    logits = model(xb)\n",
    "\n",
    "    logits = rearrange(logits, 'batch_size seq_len n_embd -> (batch_size seq_len) n_embd')\n",
    "    targets = rearrange(yb, 'batch_size seq_len -> (batch_size seq_len)')\n",
    "        \n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch('train', context_length)\n",
    "loss = compute_loss(gpt_model, xb, yb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:38:29) [Clang 13.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad3a0856b3806de976622ee269f2af8c4b8156df90189b419b77dd4e12e892ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
